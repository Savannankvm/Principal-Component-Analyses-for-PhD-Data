{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72a0a21",
   "metadata": {},
   "source": [
    "# PCA for dim. reduction of geochemical data\n",
    "__Authors:__ Matt Divers, Savanna van Mesdag  \n",
    "__Affiliation:__ University of Glasgow, Department of Geographical and Earth Sciences  \n",
    "  \n",
    "This notebook imports .csv geochemical data, converts to a data array, and performs dimensional reduction by principal component analysis (PCA) using sklearn singular value decomposition (SVD) based method. The output is then visualised to evaluate any emerging patterns.\n",
    "\n",
    "To run the notebook on new data seamlessly, the .csv should be structured as follows: \n",
    "- A row for each datapoint.  \n",
    "- The first column for location (or some other descriptor).  \n",
    "- A column per measured molecule (for geochemical data), or other dimensionality.  \n",
    "\n",
    "If your data is in another format (including file format), the code will need to be amended to suit.  \n",
    "  \n",
    "As a prerequisite for running this notebook, we recommend setting up an **anaconda** environment for Python 3 using the following commands:  \n",
    "> conda create -n geochem python=3.8  \n",
    "> conda activate geochem\n",
    "  \n",
    "> conda install -c conda-forge scikit-learn  \n",
    "> conda install -c conda-forge pandas  \n",
    "> conda install -c anaconda matplotlib  \n",
    "> conda install -c anaconda seaborn  \n",
    "> conda install -c anaconda jupyter  \n",
    "  \n",
    "> cd \"your/file/path/here\"  \n",
    "  \n",
    "> jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a332356e",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e11600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set matplotlib backend\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib # visualisation, plotting etc [1]\n",
    "import matplotlib.pyplot as plt # visualisation, plotting etc [1]\n",
    "import seaborn as sns # more plotting, better linkage with pandas [2]\n",
    "\n",
    "import pandas as pd # load .csv and dataframe management [3]\n",
    "import numpy as np # matrix mathematics [4]\n",
    "\n",
    "import sklearn as skl # machine learning, i.e. PCA etc... [5]\n",
    "from sklearn.decomposition import PCA # PCA [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3f441b",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a505b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Plant_Chemistry_Major_and_Trace_Elements.csv') # read data as pandas dataframe\n",
    "display(df) # display the table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f55a3",
   "metadata": {},
   "source": [
    "Above, we can see all the data as a table. Each row represents a sample, and each column represents a dimension to the data. We want only the numerical data, so we reformat the data as a NumPy array and drop the first column representing location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547bdf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create np array of data excluding location\n",
    "data = np.array(df.iloc[:, 2 : ]) # df.iloc[EVERY ROW, 3rd COLUMN : to LAST COLUMN]... python counts from 0 so 1 is 2nd column\n",
    "print(data.shape) # print the shape of the data array\n",
    "print(data.dtype) # print the data type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e23ce8",
   "metadata": {},
   "source": [
    "All looks well, data has 41 samples and 13 dimensions (i.e., the measured molecules). The data is also in floating point format, which is what is required as an input for scikit-learn PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869e7c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's preserve a variable for number of samples (n_row) and number of dimensions (n_col)\n",
    "n_row, n_col = data.shape # as above, we see data.shape outputs a tuple. So we assign one value for each shape dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3a111",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot data as heatmap\n",
    "plt.figure(figsize = (8,6), dpi=600) # create figure\n",
    "plt.title('Raw data as heatmap') # set title\n",
    "# plot data as image\n",
    "im = plt.pcolormesh(data, cmap='viridis', vmin=0, vmax=100) # use vmin and vmax to set bounds for 100%\n",
    "plt.colorbar(mappable=im, ax=plt.gca(), label='weight %') # add colorbar\n",
    "\n",
    "# get current axis\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.set_box_aspect(1) # alter aspect ratio\n",
    "\n",
    "# x ticks settings\n",
    "x_tcks = np.arange(0, n_col, 1) # create an array of 1 to n dimensions\n",
    "x_tck_labels = []\n",
    "for comp in df.iloc[:,2:].columns: # use compositions as labels from data frame column names\n",
    "    x_tck_labels.append(comp)\n",
    "\n",
    "ax.set_xticks(x_tcks) # create x tick for each dimension\n",
    "ax.set_xticklabels(x_tck_labels, rotation = 90, ha = 'left') # add label for each\n",
    "\n",
    "# y tick settings\n",
    "y_tcks = np.arange(0, n_row, 1) # create an array of 1 to n samples\n",
    "y_tck_labels = df.iloc[:, 0]\n",
    "\n",
    "ax.set_yticks(y_tcks) # create x tick for each dimension\n",
    "ax.set_yticklabels(y_tck_labels, ha = 'right', va = 'bottom', fontsize = 8) # add label for each\n",
    "\n",
    "plt.grid(True, color='black')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.savefig(f'figs/RawDataHeatmap.tif', format='tif')\n",
    "\n",
    "plt.close(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c82560",
   "metadata": {},
   "source": [
    "From above, we see can start to see which molecules show the most significant variation. Now let's try to statistically simplify the complexity of the problem, and try to identify correlation in the dataspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d90f450",
   "metadata": {},
   "source": [
    "## Sklearn PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b989f421",
   "metadata": {},
   "source": [
    "Initial run, we do not define a smaller number of components than those in the data. This will allow us to determine the optimum heuristically with a scree plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152709aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since scikit-learn PCA is an SVD-based implementation, we must first center the data or else end up with a non-centered PCA\n",
    "data_pca = skl.preprocessing.scale(data, axis=1) # set axis equal to the dimension axis (in this case 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6510a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(random_state = 0) # create our linear decomposition model, random state for reproducibility\n",
    "pca.fit(data_pca) # fit model to data\n",
    "\n",
    "exp_var = pca.explained_variance_ratio_*100 # get explained variance (as a %)\n",
    "csum_var = np.cumsum(exp_var) # cumulative explained variance\n",
    "\n",
    "print(f'Total explained variance = {exp_var.sum()}') # check explained variance sums to 100%... with some rounding errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eab66c",
   "metadata": {},
   "source": [
    "We want to determine an optimal number of components in the data. This is done heuristically with a scree plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa0ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=600)\n",
    "plt.title('Cumulative explained variance plot')\n",
    "\n",
    "plt.plot(exp_var, marker = 'o', label = 'successive') # plot non cumulative explained variance\n",
    "plt.plot(csum_var, marker = 'x', label = 'cumulative') # plot cumulative explained variance\n",
    "\n",
    "plt.xticks(np.arange(0,n_col,2), np.arange(1,n_col+1,2))\n",
    "\n",
    "plt.xlabel('Principal components')\n",
    "plt.ylabel('Explained variance (%)')\n",
    "\n",
    "plt.legend() # add legend\n",
    "\n",
    "plt.savefig(f'figs/PCA_ScreePlot.tif', format='tif')\n",
    "\n",
    "plt.close(fig=plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c05fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'2 component explained variance = {csum_var[1]} %') # cumulative sum of 2 components (python counts from 0 remember)\n",
    "print(f'3 component explained variance = {csum_var[2]} %') # cumulative sum of 3 components\n",
    "print(f'4 component explained variance = {csum_var[3]} %') # cumulative sum of 4 components\n",
    "print(f'Maximum variance beyond 4 components = {exp_var[4:].max()} %') # if PCA is correctly computed, the max variance beyond 3 components should be 4th component. Good to check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db62cb",
   "metadata": {},
   "source": [
    "We are looking for the \"elbow\" in the scree plot to determine the optimal number of components. This may exist at a few points (2, 3, or 4 components). We see that component 3 explains ~5% of variance, making it significant enough. 4th component and beyond only describe <1% of the total variance. Therefore, **4 components are optimal**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c259ce28",
   "metadata": {},
   "source": [
    "### Re-run PCA with optimal *n_comps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b3d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comps = 4 # choose optimal number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba1244",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = n_comps, random_state = 0) # create our dim reduction model, random state for reproducibility\n",
    "pca.fit(data_pca) # fit model to data\n",
    "\n",
    "exp_var = pca.explained_variance_ratio_*100 # get explained variance (as a %)\n",
    "csum_var = np.cumsum(exp_var) # cumulative explained variance\n",
    "\n",
    "print(f'Total explained variance = {exp_var.sum()}') # check explained variance sums to same value as earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd67faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pca.transform(data_pca) # get score matrix \n",
    "print(scores.shape)\n",
    "\n",
    "loads = pca.components_# get loadings matrix\n",
    "print(loads.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357f7808",
   "metadata": {},
   "source": [
    "Now our reduced dimensional model is divided into a scores matrix, and loadings matrix. Scores matrix is of shape [number of samples, number of principal components], whilst loadings matrix is of shape [number of components, number of input dimensions]. In this case, the scores matrix describes positive/negative correlation each sample has with each principal component, and the loadings matrix describes the positive/negative correlation each principal component has with each dimension.  \n",
    "  \n",
    "We can consider the **loadings matrix** a **reduction in the number of samples** (i.e., if we only had *n_comps* number of \"samples\", which \"samples\" would explain most of the variation), whilst the **scores matrix** represents a **dimensionality reduction** - each sample can now be desceribed in only *n_comps* number of dimensions, rather than the original input number of dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455654d4",
   "metadata": {},
   "source": [
    "#### Visualisation\n",
    "First, let's plot the loadings matrix as though it were *n_comps* number of new samples. We still have the original number of dimensions, but now we have *n_comps* number of \"samples\" which explain almost the same amount of variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5654c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### visualise the loadings matrix\n",
    "# plot data as heatmap\n",
    "plt.figure(figsize = (8,6), dpi=600) # create figure\n",
    "plt.title('PCA loadings matrix') # set title\n",
    "\n",
    "plt.pcolormesh(loads, cmap = 'viridis', vmin=loads.min(), vmax=loads.max()) # plot data as image\n",
    "plt.colorbar(location='bottom', label='loading value') # add colorbar\n",
    "\n",
    "# get current axis\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.set_box_aspect(n_comps/n_col) # alter aspect ratio\n",
    "ax.invert_yaxis() # flip y axis\n",
    "\n",
    "# x ticks settings\n",
    "x_tcks = np.arange(0, n_col, 1) # create an array of 1 to 13 (n dimensions)\n",
    "x_tck_labels = []\n",
    "for comp in df.iloc[:,2:].columns: # use compositions as labels from data frame column names\n",
    "    x_tck_labels.append(comp)\n",
    "\n",
    "ax.set_xticks(x_tcks) # create x tick for each dimension\n",
    "ax.set_xticklabels(x_tck_labels, rotation = 90, ha = 'left') # add label for each molecule\n",
    "\n",
    "# y tick settings\n",
    "y_tcks = np.arange(0, n_comps, 1) # create an array of 1 to 3 (n components)\n",
    "y_tck_labels = []\n",
    "for i in range(1,n_comps+1):\n",
    "    y_tck_labels.append(f'PC {i}')\n",
    "\n",
    "ax.set_yticks(y_tcks) # create x tick for each dimension\n",
    "ax.set_yticklabels(y_tck_labels, ha = 'right', va = 'top', fontsize = 14) # add label for each\n",
    "\n",
    "plt.grid(True, color='black')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.savefig(f'figs/PCA4_LoadingsHeatmap.tif', format='tif')\n",
    "\n",
    "plt.close(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e9abd",
   "metadata": {},
   "source": [
    "This visualisation helps us understand the assumptions of PCA. The columns \"MgO\" to \"BaO\" exist in trace abundance only, and as such have very little weight when it comes to supplying variance to the dataset. The first 3 components instead are explaining:\n",
    " - 1. High SiO2 but low CaO, everthing else very moderate.  \n",
    " - 2. High SiO2 and high CaO, slightly lower Al2O3, everything else very moderate.  \n",
    " - 3. Moderate amounts of everything except high Fe2O3.  \n",
    "*Note: \"high\"/\"low\" is relative to the rest of the dataset, not as weight %.*  \n",
    "  \n",
    "Now let's see how that affects the distribution of our components in each of the original samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdcd9ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot data as heatmap\n",
    "plt.figure(figsize = (4,8), dpi=600) # create figure\n",
    "plt.title('PCA scores matrix') # set title\n",
    "# plot data as image\n",
    "im = plt.pcolormesh(scores, cmap='viridis', vmin=scores.min(), vmax=scores.max())\n",
    "plt.colorbar(mappable=im, ax=plt.gca(), label='PCA scores') # add colorbar\n",
    "\n",
    "# get current axis\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.set_box_aspect(n_row/(n_comps*2)) # alter aspect ratio\n",
    "\n",
    "# x tick settings\n",
    "x_tcks = np.arange(0, n_comps, 1) # create an array of 1 to 3 (n components)\n",
    "x_tck_labels = []\n",
    "for i in range(1, n_comps+1):\n",
    "    x_tck_labels.append(f'PC {i}')\n",
    "\n",
    "ax.set_xticks(x_tcks) # create x tick for each dimension\n",
    "ax.set_xticklabels(x_tck_labels, rotation = 45, ha = 'left') # add label for each\n",
    "\n",
    "# y tick settings\n",
    "y_tcks = np.arange(0, n_row, 1) # create an array of 1 to n samples\n",
    "y_tck_labels = df.iloc[:, 0]\n",
    "\n",
    "ax.set_yticks(y_tcks) # create x tick for each dimension\n",
    "ax.set_yticklabels(y_tck_labels, ha = 'right', va = 'bottom', fontsize = 8) # add label for each\n",
    "\n",
    "plt.grid(True, color='black')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.savefig(f'figs/PCA4_ScoresHeatmap.tif')\n",
    "\n",
    "plt.close(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e67902f",
   "metadata": {},
   "source": [
    "Finally let's try to better represent the dimensionally reduced dataspace with scatter plots and histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c549dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### creating a colorblind friendly color map\n",
    "# create a float RGBA (red, green, blue, alpha) color for each category\n",
    "colors_cbf = np.array((np.array([35/255, 187/255, 20/255, 1]), np.array([216/255, 27/255, 96/255, 1]), \n",
    "                   np.array([255/255, 193/255, 7/255, 1]), np.array([0/255, 77/255, 64/255, 1]),\n",
    "                   np.array([102/255, 179/255, 225/255, 1]), np.array([77/255, 11/255, 131/255, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a key system for the location name only, by using .split()\n",
    "location = [] # create an empty list to append to\n",
    "\n",
    "for comm in df.loc[:, 'Community']: # for each entry in the 'Community' column of the dataframe\n",
    "    location.append(comm.split(sep='_')[0]) # the separator is '_'\n",
    "\n",
    "# get list of unique locations\n",
    "loc_unique = np.unique(location)\n",
    "# create list of matplotlib markers for each category\n",
    "marker_lst = ['o', '^', 's', '*', 'D', 'X']\n",
    "\n",
    "for i in range(0, len(loc_unique)): # for each number of categories\n",
    "    print(f'{loc_unique[i]} : {marker_lst[i]}') # print unique place name : marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c88bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Visualise the scores\n",
    "# histogram of scores\n",
    "fig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4, figsize = (8,2))\n",
    "\n",
    "ax1.hist(scores[:,0], bins = 25, color = 'red', edgecolor = 'black') # create hist for 1st PC score\n",
    "ax1.set_title('PCA score 1')\n",
    "\n",
    "ax2.hist(scores[:,1], bins = 25, color = 'green', edgecolor = 'black') # create hist for 2nd PC score\n",
    "ax2.set_title('PCA score 2')\n",
    "\n",
    "ax3.hist(scores[:,2], bins = 25, color = 'blue', edgecolor = 'black') # create hist for 3rd PC score\n",
    "ax3.set_title('PCA score 3')\n",
    "\n",
    "ax4.hist(scores[:,3], bins = 25, color = 'gray', edgecolor = 'black') # create hist for 3rd PC score\n",
    "ax4.set_title('PCA score 4')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# scatter plots\n",
    "fig, ((ax1,ax2,ax3),(ax4,ax5,ax6)) = plt.subplots(2,3, figsize = (9,5))\n",
    "\n",
    "fig.suptitle('PCA scores')\n",
    "\n",
    "sns.scatterplot(x=scores[:,0], y=scores[:,1], alpha = 0.75, hue = location, ax = ax1,\n",
    "                palette=colors_cbf, edgecolor = 'black', legend=False, style=location, markers=marker_lst) # scatter PC 1vs2\n",
    "ax1.set_xlabel('PC 1')\n",
    "ax1.set_ylabel('PC 2')\n",
    "\n",
    "sns.scatterplot(x=scores[:,0], y=scores[:,2], alpha = 0.75, hue = location, ax = ax2,\n",
    "                palette=colors_cbf, edgecolor = 'black', legend=False, style=location, markers=marker_lst) # scatter PC 1vs3\n",
    "ax2.set_xlabel('PC 1')\n",
    "ax2.set_ylabel('PC 3')\n",
    "\n",
    "sns.scatterplot(x=scores[:,0], y=scores[:,3], alpha = 0.75, hue = location, ax = ax3,\n",
    "                palette=colors_cbf, edgecolor = 'black', legend=False, style=location, markers=marker_lst) # scatter PC 1vs4\n",
    "ax3.set_xlabel('PC 1')\n",
    "ax3.set_ylabel('PC 4')\n",
    "\n",
    "sns.scatterplot(x=scores[:,1], y=scores[:,2], alpha = 0.75, hue = location, ax = ax4,\n",
    "                palette=colors_cbf, edgecolor = 'black', legend=False, style=location, markers=marker_lst) # scatter PC 2vs3\n",
    "ax4.set_xlabel('PC 2')\n",
    "ax4.set_ylabel('PC 3')\n",
    "\n",
    "sns.scatterplot(x=scores[:,1], y=scores[:,3], alpha = 0.75, hue = location, ax = ax5,\n",
    "                palette=colors_cbf, edgecolor = 'black', legend=False, style=location, markers=marker_lst) # scatter PC 2vs4\n",
    "ax5.set_xlabel('PC 2')\n",
    "ax5.set_ylabel('PC 4')\n",
    "\n",
    "sns.scatterplot(x=scores[:,2], y=scores[:,3], alpha = 0.75, hue = location, ax = ax6,\n",
    "                palette=colors_cbf, edgecolor = 'black', legend=False, style=location, markers=marker_lst) # scatter PC 3vs4\n",
    "ax6.set_xlabel('PC 3')\n",
    "ax6.set_ylabel('PC 4')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.savefig('figs/PCA4_ScoresScatter_subplots.tif', format='tif')\n",
    "\n",
    "plt.close(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a custom legend for plt.legend()\n",
    "leg = [] # create empty list\n",
    "for i in range(0, len(loc_unique)): # for each unique location\n",
    "    leg.append(matplotlib.lines.Line2D([], [], marker = marker_lst[i], color = colors_cbf[i], linestyle = 'None')) # create empty line with custom markers and colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e1f62d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create plot for each combination of components\n",
    "for a in range(0, n_comps): # from 0 to n_comps\n",
    "    for b in range(0, n_comps): # from 0 to n_comps again\n",
    "        if a<=b: # remove repeats and straight lines\n",
    "            pass\n",
    "        else: # otherwise create scatter plot\n",
    "            plt.figure(dpi=600)\n",
    "            ax = plt.gca()\n",
    "            sns.scatterplot(x=scores[:,a], y=scores[:,b], alpha = 0.75, hue = location, ax = ax,\n",
    "                            palette=colors_cbf, edgecolor = 'black', legend=False, style=location, \n",
    "                            markers=marker_lst)\n",
    "            # add custom legend\n",
    "            plt.legend(leg, loc_unique, loc = (1.04, 0.18), markerscale = 1.5, fontsize = 8, \n",
    "                       labelspacing = 1)\n",
    "            # set x and y labels\n",
    "            plt.xlabel(f'PC {a+1}', fontsize=12, fontweight='bold')\n",
    "            plt.ylabel(f'PC {b+1}', fontsize=12, fontweight='bold')\n",
    "            \n",
    "            ax.set_box_aspect(1)\n",
    "            \n",
    "            plt.savefig(f'figs/scores_scatter_PC{a+1}vsPC{b+1}.tif', format='tif')\n",
    "            \n",
    "            plt.close(fig=plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71388c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### now with \"correct\" aspect ratio\n",
    "# create plot for each combination of components\n",
    "for a in range(0, n_comps): # from 0 to n_comps\n",
    "    for b in range(0, n_comps): # from 0 to n_comps again\n",
    "        if a<=b: # remove repeats and straight lines\n",
    "            pass\n",
    "        else: # otherwise create scatter plot\n",
    "            plt.figure(dpi=600)\n",
    "            ax = plt.gca()\n",
    "            ax.set_box_aspect((scores[:,b].max()-scores[:,b].min())/(scores[:,a].max()-scores[:,a].min())) # set aspect using range of values in each dimension\n",
    "            sns.scatterplot(x=scores[:,a], y=scores[:,b], alpha = 0.75, hue = location, ax = ax,\n",
    "                            palette=colors_cbf, edgecolor = 'black', legend=False, style=location, \n",
    "                            markers=marker_lst)\n",
    "            # add custom legend\n",
    "            plt.legend(leg, loc_unique, loc = (1.04, 0.18), markerscale = 1.5, fontsize = 8, \n",
    "                       labelspacing = 1)\n",
    "            # set x and y labels\n",
    "            plt.xlabel(f'PC {a+1}', fontsize=12, fontweight='bold')\n",
    "            plt.ylabel(f'PC {b+1}', fontsize=12, fontweight='bold')\n",
    "            \n",
    "            plt.savefig(f'figs/scores_scatter_CorrectedAspectRatio_PC{a+1}vsPC{b+1}.tif', format='tif')\n",
    "            \n",
    "            plt.close(fig=plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d2335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change backend for interactive plotting\n",
    "%matplotlib notebook \n",
    "\n",
    "fig = plt.figure(figsize=(8,8)) # create figure\n",
    "ax = fig.add_subplot(projection='3d', ) # add subplot for 3d plotting\n",
    "\n",
    "scatter = ax.scatter(scores[:,0], scores[:,1], scores[:,2], c=scores[:,3], cmap='viridis', s=40, edgecolor='black') # add scatter data for principal components 1, 2, & 3 (x, y, z)\n",
    "cb = plt.colorbar(mappable=scatter, ax=ax, location='bottom')\n",
    "cb.set_label(label='PC 4', weight='bold')\n",
    "\n",
    "# set axis limits\n",
    "ax.set_xlim(scores.min(), scores.max())\n",
    "ax.set_ylim(scores.min(), scores.max())\n",
    "ax.set_zlim(scores.min(), scores.max())\n",
    "\n",
    "# add axis labels\n",
    "ax.set_xlabel('PC 1', fontweight='bold')\n",
    "ax.set_ylabel('PC 2', fontweight='bold')\n",
    "ax.set_zlabel('PC 3', fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd149fd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "\n",
    "fig = plt.figure(figsize=(8,8)) # create figure\n",
    "ax = fig.add_subplot(projection='3d', ) # add subplot for 3d plotting\n",
    "\n",
    "# set axis limits\n",
    "ax.set_xlim(scores.min(), scores.max())\n",
    "ax.set_ylim(scores.min(), scores.max())\n",
    "ax.set_zlim(scores.min(), scores.max())\n",
    "\n",
    "# add axis labels\n",
    "ax.set_xlabel('PC 1', fontweight='bold')\n",
    "ax.set_ylabel('PC 2', fontweight='bold')\n",
    "ax.set_zlabel('PC 3', fontweight='bold')\n",
    "\n",
    "idx = 0\n",
    "for loc in location:\n",
    "    loc_idx = 0\n",
    "    for locU in loc_unique:\n",
    "        if loc == locU:\n",
    "            ax.scatter(scores[idx,0], scores[idx,1], scores[idx,2], color=colors_cbf[loc_idx], s=40, \n",
    "                       marker=marker_lst[loc_idx], edgecolor='black', depthshade=True)\n",
    "            idx+=1\n",
    "        else:\n",
    "            loc_idx+=1\n",
    "    \n",
    "#scatter = ax.scatter(scores[:,0], scores[:,1], scores[:,2], c=scores[:,3], cmap='viridis', s=1, marker=',') # add scatter data for principal components 1, 2, & 3 (x, y, z)\n",
    "#cb = plt.colorbar(mappable=scatter, ax=ax, location='bottom')\n",
    "#cb.set_label(label='PC 4', weight='bold')\n",
    "\n",
    "plt.legend(leg, loc_unique, markerscale = 2, fontsize = 10, labelspacing = 1.25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b6df56",
   "metadata": {},
   "source": [
    "The scatter plots and histograms above highlight the fact that there is not enough samples for a robust cluster analysis. There are several areas where points are deviating from one another; however, it is hard to know if these points are noise in the data, or if with a larger dataset these would form groups of their own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc0070",
   "metadata": {},
   "source": [
    "### Export data\n",
    "Convert loadings and score matrices back into a dataframe with the appropriate headers, and save as .csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f37cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x ticks settings\n",
    "x_tcks = np.arange(0, n_col, 1) # create an array of 1 to 13 (n dimensions)\n",
    "x_tck_labels = []\n",
    "for comp in df.iloc[:,2:].columns: # use compositions as labels from data frame column names\n",
    "    x_tck_labels.append(comp)\n",
    "    \n",
    "# y tick settings\n",
    "y_tcks = np.arange(0, n_comps, 1) # create an array of 1 to 3 (n components)\n",
    "y_tck_labels = []\n",
    "for i in range(1, n_comps+1):\n",
    "    y_tck_labels.append(f'PC {i}')\n",
    "\n",
    "# create pandas dataframe for exporting\n",
    "df_loads = pd.DataFrame(data = loads, index = y_tck_labels, columns = x_tck_labels) # create df with data as loadings matrix, and appropriate row and column labels\n",
    "df_loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90796b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loads.to_csv('PlantChemistry_MajorTraceElements_PCA4_loadings.csv') # save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca3fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(data = scores, columns = y_tck_labels) # create df with data as score matrix, and appropriate column labels\n",
    "df_scores['Community'] = df.loc[:,'Community'] # add locations column\n",
    "\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f09061",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv('PlantChemistry_MajorTraceElements_PCA4_scores.csv') # save as csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ac4541",
   "metadata": {},
   "source": [
    "## References\n",
    "#### Plotting  \n",
    "> [1] J.D. Hunter. 2007. Matplotlib: A 2D graphics environment. *Computing in Science & Engineering*, **9**(3), pp.90-95, doi:10.1109/MCSE.2007.55    \n",
    "  \n",
    "> [2] M.L. Waskom. 2021. seaborn: statistical data vizualisation. *The Open Journal*, **6**(60), pp. 3021, doi:10.21105/joss.03021  \n",
    "  \n",
    "#### Data management    \n",
    "> [3] The pandas development team. 2020. pandas-dev/pandas: Pandas. *Zenodo*, doi:10.5281/zenodo.3509134     \n",
    "  \n",
    "> [4] C.R. Harris, K.J. Millman, S.J. van der Walt, *et al*. 2020. Array programming with NumPy. *Nature* **585**, pp. 357â€“362, doi:10.1038/s41586-020-2649-2  \n",
    "  \n",
    "#### Principal component analysis   \n",
    "> [5] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, & E. Duchesnay. 2011. Scikit-learn: Machine Learning in {P}ython, *Journal of Machine Learning Research*, **12**, pp. 2825-2830."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
